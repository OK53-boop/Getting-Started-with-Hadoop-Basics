1.
配置本机主机名为 hadoop，sudo 时需要输入 shiyanlou 用户的密码。将 hadoop 添加到最后一行的末尾。

sudo vim /etc/hosts
# 将hadoop添加到最后一行的末尾，修改后类似：（使用 tab 键添加空格）
# 172.17.2.98 f738b9456777 hadoop
ping hadoop

使用如下命令启动 Hadoop：

cd /app/hadoop-1.1.2/bin
./start-all.sh
jps # 查看启动的进程，确保 NameNode 和 DataNode 都有启动

测试数据包括两个文件 dept（部门）和 emp（员工），其中各字段用逗号分隔：
请结合生活中的例子详细分析一下。

好的，让我结合一个生活中的例子来详细分析一下。

假设我们是一家电商公司，我们需要对我们的销售数据进行分析以便更好地了解我们的客户和市场趋势。我们可以使用Hadoop系统来处理和分析大量的销售数据。

我们可以销售数据存储在两个文件中，一个文件包含有关部门的信息，另一个文件包含有关员工的信息。每个文件中的字段都用逗号分隔，例如，部门文件中的字段可能包括部门ID、部门名称和部门经理，而员工文件的字段可能包括员工ID、员工姓名、所属部门和销售额等信息。

我们可以使用Hadoop系统来处理这些文件，以便进行各种分析，例如，我们可以使用MapReduce程序来计算每个部门的总销售额，或者计算每个员工的平均销售额。我们还可以使用Hive或Pig等工具来执行更高级的分析，例如，使用机器学习算法来预测未来的销售趋势。

作为系统工程师，我们需要确保Hadoop系统已正确配置，并且正在处理的数据准确且与预期用例相关。我们还需要确保系统能够处理大量的数据，并且能够在合理的时间内完成分析任务。我们还需要考虑系统的可靠性和安全性，以确保数据不会丢失或被未经授权的人访问。





2.
配置主机名为 hadoop，sudo 时需要输入 shiyanlou 用户的密码。将 hadoop 添加到最后一行的末尾。

sudo vim /etc/hosts
# 将hadoop添加到最后一行的末尾，修改后类似：（使用 tab 键添加空格）
# 172.17.2.98 f738b9456777 hadoop
ping hadoop

使用如下命令启动 Hadoop：

cd /app/hadoop-1.1.2/bin
./start-all.sh
jps # 查看启动的进程，确保 NameNode 和 DataNode 都有启动

测试数据包括两个文件 dept（部门）和 emp（员工），其中各字段用逗号分隔：

dept 文件内容：

10,ACCOUNTING,NEW YORK
20,RESEARCH,DALLAS
30,SALES,CHICAGO
40,OPERATIONS,BOSTON

emp 文件内容：

7369,SMITH,CLERK,7902,17-12月-80,800,,20
7499,ALLEN,SALESMAN,7698,20-2月-81,1600,300,30
7521,WARD,SALESMAN,7698,22-2月-81,1250,500,30
7566,JONES,MANAGER,7839,02-4月-81,2975,,20
7654,MARTIN,SALESMAN,7698,28-9月-81,1250,1400,30
7698,BLAKE,MANAGER,7839,01-5月-81,2850,,30
7782,CLARK,MANAGER,7839,09-6月-81,2450,,10
7839,KING,PRESIDENT,,17-11月-81,5000,,10
7844,TURNER,SALESMAN,7698,08-9月-81,1500,0,30
7900,JAMES,CLERK,7698,03-12月-81,950,,30
7902,FORD,ANALYST,7566,03-12月-81,3000,,20
7934,MILLER,CLERK,7782,23-1月-82,1300,,10

在 /home/shiyanlou/install-pack/class6 目录可以找到这两个文件，把这两个文件上传到 HDFS 中 /class6/input 目录中，执行如下命令：

cd /home/shiyanlou/install-pack/class6
hadoop fs -mkdir -p /class6/input
hadoop fs -copyFromLocal dept /class6/input
hadoop fs -copyFromLocal emp /class6/input
hadoop fs -ls /class6/input

请结合生活中的例子详细分析一下。

好的，让我结合一个生活中的例子来详细分析一下。

假设我们是一家电商公司，我们需要对我们的销售数据进行分析以便更好地了解我们的客户和市场趋势。我们可以Hadoop系统来处理和分析大量的销售数据。

在这个例子中，我们有两个文件，一个包含有关部门的信息另一个包含有关员工的信息。我们需要将这些文件上传到Hadoop分布式文件系统（HDFS）中，以便我们可以使用adoop系统来处理和分析这些数据。

我们可以使用以下命令将这些文件上传到HDFS中：

cd /home/shiyanlou/install-pack/class6
hadoop fs -mkdir -p /class6/input
hadoop fs -copyFromLocal dept /class6/input
hadoop fs -copyFromLocal emp /class6/input
hadoop fs -ls /class6/input

第一行命令创建了一个名为 /class6/input 的目录，如果该目录不存在，则会自动创建。第二行和第三行命令将 dept 和 emp 文件从本地文件系统复制到 HDFS 中的 /class6/input 目录中。最后一行命令用于验证文件是否已成功上传到 HDFS 中。

一旦文件已经上传到 HDFS 中，我们可以使用Hadoop系统来处理和分析这些数据。例如，我们可以使用MapReduce程序来计算每个部门的总销售额，或者计算每个员工的平均销售额。我们还可以使用Hive或Pig等工具来执行更高级的分析，例如，使用机器学习算法来预测未来的销售趋势。

作为系统工程师，我们需要确保Hadoop系统已正确配置，并且正在处理的数据准确且与期用例相关。我们还需要确保系统能够处理大量的数据，并且能够在合理的时间内完成分析任务。我们还需要考系统的可靠性和安全性，以确保数据不会丢失或被未经授权的人访问。



3.
问题分析

MapReduce 中的 join 分为好几种，比如有最常见的 reduce side join、map side join 和 semi join 等。reduce join 在 shuffle 阶段要进行大量的数据传输，会造成大量的网络 IO 效率低下，而 map side join 在处理多个小表关联大表时非常有用 。
请结合生活中的例子详细分析一下。

好的，让我结合一个生活中的例子来详细分析一下。

假设我们是一家电商公司，我们需要对我们的销售数据进行分析以便更好地了解我们的客户和市场趋势。我们有两个数据集，一个包含有关客户的信息，另一个包含有关订单的信息。我们需要将这两个数据集进行关联，以便我们可以了解每个客户的订单情况。

在这个例子中，我们可以使用MapReduce中的join操作来将这两个数据集进行关联。reduce side join是最常见的join操作，它需要在shuffle阶段进行大量的数据传输，这会导致网络IO效率低下。但是，如果我们使用map side join，我们可以避免这种情。

在map side join中，我们可以将一个小的数据集加载到内存中，并将其作为缓存。然后，我们可以将另一个大数据集分成小块，并将每个块发送到不同的map任务中。每个map任务都可以使用缓存中的小数据集来进行关联操作，这样就避免了大量的数据传输和网络IO。

在我们的电商公司的例子中，我们可以将客户数据集加载到内存中，并将订单数据集分成小块。然后，我们可以将每个订单块发送到不同的map任务中，并使用缓存中的客户数据集来进行关联操作。这样，我们就可以避免大量的数据传输和网络IO，从而提高MapReduce作业的效率。

除了map side join，还有其他类型的join操作，例如semi join。semi join可以用于过滤掉不需要的数据，从而减少数据传输和处理的时间。在我们的电商公司的例子中，我们可以使用semi join来过滤掉不需要的订单数据，例如已取消的订单或已退货的订单。这样，我们就可以减少数据传输和处理的时间，从而提高MapReduce作业的效率。




4.
Map side join 是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样我们可以将小表复制多份，让每个 map task 内存中存在一份（比如存放到 hash table 中），然后只扫描大表，对于大表中的每一条记录 key/value，在 hash table 中查找是否有相同的 key 的记录，如果有，则连接后输出即可。为了支持文件的复制，Hadoop 提供了一个类 DistributedCache，使用该类的方法如下：

用户使用静态方法 DistributedCache.addCacheFile() 指定要复制的文件，它的参数是文件的 URI（如果是 HDFS 上的文件，可以这样：hdfs://jobtracker:50030/home/XXX/file）。JobTracker 在作业启动之前会获取这个 URI 列表，并将相应的文件拷贝到各个 TaskTracker 的本地磁盘上。
用户使用 DistributedCache.getLocalCacheFiles() 方法获取文件目录，并使用标准的文件读写 API 读取相应的文件。

请结合生活中的例子详细分析一下。

好的，让我再结合一个生活中的例子来详细分析一下。

假设我们是一家在线教育公司，我们需要对我们学生数据进行分析以便更好地了解我们的学生和市场趋势。我们有两个数据集，一个包含有关学生信息，另一个包含有关课程的信息。我们需要将这两个数据集进行关联，以便我们可以了解每个学生的课程情况。

这个例子中，我们可以使用MapReduce中的join操作来将这两个数据集进行关联。由于学生数据集通常比课程数据集小得多，我们可以使用map side join来优化这个过程。

首先，我们可以将学生数据集加载到内存中，并将其作为缓存。然，我们可以将课程数据集分成小块，并将每个块发送到不同的map任务中。每个map任务都可以使用缓存中的生数据集来进行关联操作，这样就避免了大量的数据传输和网络IO。

为了支持文件的复制，我们可以使用Hadoop提供的DistributedCache类。我们可以使用静态方法DistributedCache.addCacheFile()指定要复制的文件，它的参数是文件的URI。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。

在我们的在线教育公司的例子中，我们可以将学生数据集存储在HDFS上，并使用DistributedCache.addCacheFile()方法将其复制到各个TaskTracker的本地磁盘上。然后，我们可以将课程数据集分成小块，并将每个块发送到不同的map任务中。每个map任务都可以使用缓存中的学生数据集来进行关联操作，这样就避免了大量的数据传输和网络IO。

最后，我们可以使用DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。这样，我们就可以在MapReduce作业中使用map side join来优化数据关联操作，从而提高作业的效率。






5.
在下面代码中，将会把数据量小的表（部门 dept) 缓存在内存中，在 Mapper 阶段对员工部门编号映射成部门名称，该名称作为 key 输出到 Reduce 中，在 Reduce 中按照部门计算各个部门的总工资。
请结合生活中的例子详细分析一下。

好的，让我结合一个生活中的例子来详细分析一下。

假设我们是一家零售公司，我们需要对我们的销售数据进行分析以便更好地了解我们的销售情况。我们有两个数据集，一个包含有关员工信息，另一个包含有关部门的信息。我们需要将这两个数据集进行关联，以便我们可以了解每个部门的销售情况。

这个例子中，我们可以使用MapReduce中的join操作来将这两个数据集进行关联。由于部门数据集通常比员工数据集小得多，我们可以使用map side join来优化这个过程。

首先，我们可以将部门数据集加载到内存中，并将其作为缓存。然后，我们可以将员工数据集分成小块，并将每块发送到不同的map任务中。每个map任务都可以使用缓存中的部门数据集来进行关联操作，这样就避免了量的数据传输和网络IO。

在Mapper阶段，我们可以将员工部门编号映射成部门名称，并将部门名称作为key输出到Reduce中。在Reduce中，我们可以按照部门计算各个部门的总工资。

为了支持文件的复制，我们可以使用Hadoop提供的DistributedCache类。我们可以使用静态方法DistributedCache.addCacheFile()指定要复制的文件，它的参数是文件的URI。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。

在我们的零售公司的例子中，我们可以将部门数据集存储在HDFS上，并使用DistributedCache.addCacheFile()方法将其复制到各个TaskTracker的本地磁盘上。然后，我们可以将员工数据集分成小块，并将每个块发送到不同的map任务中。每个map任务都可以使用缓存中的部门数据集来进行关联操作，这样就避免了大量的数据传输和网络IO。

最后，在Reduce阶段，我们可以按照部门计算各个部门的总工资。这样，我们就可以在MapReduce作业中使用map side join来优化数据关联操作，从而提高作业的效率。





6.
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q1SumDeptSalary extends Configured implements Tool {

    public static class MapClass extends Mapper<LongWritable, Text, Text, Text> {

        // 用于缓存 dept文件中的数据
        private Map<String, String> deptMap = new HashMap<String, String>();
        private String[] kv;

        // 此方法会在Map方法执行之前执行且执行一次
        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
            BufferedReader in = null;
            try {

                // 从当前作业中获取要缓存的文件
                Path[] paths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
                String deptIdName = null;
                for (Path path : paths) {

                    // 对部门文件字段进行拆分并缓存到deptMap中
                    if (path.toString().contains("dept")) {
                        in = new BufferedReader(new FileReader(path.toString()));
                        while (null != (deptIdName = in.readLine())) {

                            // 对部门文件字段进行拆分并缓存到deptMap中
                            // 其中Map中key为部门编号，value为所在部门名称
                            deptMap.put(deptIdName.split(",")[0], deptIdName.split(",")[1]);
                        }
                    }
                }
            } catch (IOException e) {
                e.printStackTrace();
            } finally {
                try {
                    if (in != null) {
                        in.close();
                    }
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }

public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

            // 对员工文件字段进行拆分
            kv = value.toString().split(",");

            // map join: 在map阶段过滤掉不需要的数据，输出key为部门名称和value为员工工资
            if (deptMap.containsKey(kv[7])) {
                if (null != kv[5] && !"".equals(kv[5].toString())) {
                    context.write(new Text(deptMap.get(kv[7].trim())), new Text(kv[5].trim()));
                }
            }
        }
    }

    public static class Reduce extends Reducer<Text, Text, Text, LongWritable> {

public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

            // 对同一部门的员工工资进行求和
            long sumSalary = 0;
            for (Text val : values) {
                sumSalary += Long.parseLong(val.toString());
            }

            // 输出key为部门名称和value为该部门员工工资总和
            context.write(key, new LongWritable(sumSalary));
        }
    }

    @Override
    public int run(String[] args) throws Exception {

        // 实例化作业对象，设置作业名称、Mapper和Reduce类
        Job job = new Job(getConf(), "Q1SumDeptSalary");
        job.setJobName("Q1SumDeptSalary");
        job.setJarByClass(Q1SumDeptSalary.class);
        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);

        // 设置输入格式类
        job.setInputFormatClass(TextInputFormat.class);

        // 设置输出格式
        job.setOutputFormatClass(TextOutputFormat.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        // 第1个参数为缓存的部门数据路径、第2个参数为员工数据路径和第3个参数为输出路径
    String[] otherArgs = new GenericOptionsParser(job.getConfiguration(), args).getRemainingArgs();
    DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(), job.getConfiguration());
        FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));

        job.waitForCompletion(true);
        return job.isSuccessful() ? 0 : 1;
    }

    /**
     * 主方法，执行入口
     * @param args 输入参数
     */
    public static void main(String[] args) throws Exception {
        int res = ToolRunner.run(new Configuration(), new Q1SumDeptSalary(), args);
        System.exit(res);
    }
}
请分别详细分析每一句代码。


好的，让我来逐行分析这段代码。

```java
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
```
这部分代码是导入所需的Java类和包。

```java
public class Q1SumDeptSalary extends Configured implements Tool {
```
这里定义了一个名为Q1SumDeptSalary的类，它继承了Configured和Tool两个类。

```java
public static class MapClass extends Mapper<LongWritable, Text, Text, Text> {
```
这里定义了一个名为MapClass的静态内部类，它继承了Mapper类，并指定了输入键、输入值、输出键和输出值的类型。

```java
private Map<String, String> deptMap = new HashMap<String, String>();
private String[] kv;
```
这里定义了一个名为deptMap的HashMap对象，用于缓存部门数据集。同时，定义了一个名为kv的String数组，用于存储员工数据集中的字段。

```java
@Override
protected void setup(Context context) throws IOException, InterruptedException {
    BufferedReader in = null;
    try {
        Path[] paths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
        String deptIdName = null;
        for (Path path : paths) {
            if (path.toString().contains("dept")) {
                in = new BufferedReader(new FileReader(path.toString()));
                while (null != (deptIdName = in.readLine())) {
                    deptMap.put(deptIdName.split(",")[0], deptIdName.split(",")[1]);
                }
            }
        }
    } catch (IOException e) {
        e.printStackTrace();
    } finally {
        try {
            if (in != null) {
                in.close();
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```
这里定义了一个名为setup的方法，它会在Map方法执行之前执行且执行一次。这个方法中，我们从DistributedCache中获取缓存的部门数据集，并将其存储到deptMap中。

```java
public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
    kv = value.toString().split(",");
    if (deptMap.containsKey(kv[7])) {
        if (null != kv[5] && !"".equals(kv[5].toString())) {
            context.write(new Text(deptMap.get(kv[7].trim())), new Text(kv[5].trim()));
        }
    }
}
```
这里定义了一个名为map的方法，它会对员工数据集中的字段进行拆分，并使用缓存的门数据集进行关联操作。如果员工数据集中的部门编号在部门数据集中存在，则将员工工资和部门名称作为输出键值对写入上下文中。

```java
public static class Reduce extends Reducer<Text, Text, Text, LongWritable> {
```
这里定义了一个名为Reduce的静态内部类，它继承了Reducer类，并指定了输入键、输入值、输出键和输出值的类型。

```java
public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
    long sumSalary = 0;
    for (Text val : values) {
        sumSalary += Long.parseLong(val.toString());
    }
    context.write(key, new LongWritable(sumSalary));
}
```
这里定义了一个名为reduce的方法，它会对同一部门的员工工资进行求和，并将部门名称和该部门员工工资总和作为输出键值对写入上下文中。

```java
@Override
public int run(String[] args) throws Exception {
    Job job = new Job(getConf(), "Q1SumDeptSalary");
    job.setJobName("Q1SumDeptSalary");
    job.setJarByClass(Q1SumDeptSalary.class);
    job.setMapperClass(MapClass.class);
    job.setReducerClass(Reduce.class);
    job.setInputFormatClass(TextInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    String[] otherArgs = new GenericOptionsParser(job.getConfiguration(), args).getRemainingArgs();
    DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(), job.getConfiguration());
    FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));
    job.waitForCompletion(true);
    return job.isSuccessful() ? 0 : 1;
}
```
这里定义了一个名为run的方法，它会实例化作业对象，并设置作业名称、Mapper和Reduce类、输入格式类、输出格式、输出键和输出值的类型等参数。然后，它会从命令行参数中获取缓存的部门数据集路径、员工数据集路径和输出路径，并将它们设置为作业的输入和输出路径。最后，它会等待作业完成并返回作业的执行结果。

```java
public static void main(String[] args) throws Exception {
    int res = ToolRunner.run(new Configuration(), new Q1SumDeptSalary(), args);
    System.exit(res);
}
```
这里定义了一个名为main的方法，它会调用ToolRunner.run()方法来运行作业，并将执行结果作为程序的退出状态。





7.
编译并打包代码

进入 /app/hadoop-1.1.2/myclass/class6 目录中新建 Q1SumDeptSalary.java 程序代码（代码也可以使用 /home/shiyanlou/install-pack/class6/Q1SumDeptSalary.java 文件）。

cd /app/hadoop-1.1.2/myclass
mkdir class6 && cd class6
vi Q1SumDeptSalary.java

编译代码：

javac -classpath ../../hadoop-core-1.1.2.jar:../../lib/commons-cli-1.2.jar Q1SumDeptSalary.java

把编译好的代码打成 jar 包（如果不打成 jar 形式运行会提示 class 无法找到的错误）。

jar cvf ./Q1SumDeptSalary.jar ./Q1SumDept*.class
mv *.jar ../..
rm Q1SumDept*.class
请详细分析一下每一句话


好的，让我来逐行分析这段代码。

```bash
cd /app/hadoop-1.1.2/myclass
mkdir class6 && cd class6
vi Q1SumDeptSalary.java
```
这里进入/app/hadoop-1.1.2/myclass目录，并创建一个名为class6的目录，并进入该目。然后，使用vi编辑器创建一个名为Q1SumDeptSalary.java的Java程序。

```bash
javac -classpath ../../hadoop-core-1.1.2.jar:../../lib/commons-cli-1.2.jar Q1SumDeptSalary.java
```
里使用javac编译Q1SumDeptSalary.java程序，并使用-classpath参数指定编译时所需的类路径。这里的类路径包括hadoop-core-1.1.2.jar和commons-cli-1.2.jar两个文件。

```bash
jar cvf ./Q1SumDeptSalary.jar ./Q1SumDept*.class
```
这里使用jar命令将编译好的Q1SumDeptSalary程序打包成一个名为Q1SumDeptSalary.jar的可执行JAR文件。这里使用-cvf参数指定打包方式，并使用./Q1Sum*.class指定要打包的所有Q1SumDept开头的class文件。

```bash
mv *.jar ../..
rm Q1SumDept*.class
```
这里使用mv命令将打包好的Q1SumDeptSalary.jar文件移动到/app/hadoop-1.1.2目录下，并使用rm命令删除所有Q1SumDept开头的class文件。





8.
运行并查看结果

运行 Q1SumDeptSalary 时需要输入部门数据路径、员工数据路径和输出路径三个参数，需要注意的是 hdfs 的路径参数需要全路径，否则运行会报错：

部门数据路径：hdfs://hadoop:9000/class6/input/dept，部门数据将缓存在各运行任务的节点内容中，可以提高处理的效率。
员工数据路径：hdfs://hadoop:9000/class6/input/emp。
输出路径：hdfs://hadoop:9000/class6/out1。
运行如下命令：

cd /app/hadoop-1.1.2
hadoop jar Q1SumDeptSalary.jar Q1SumDeptSalary hdfs://hadoop:9000/class6/input/dept hdfs://hadoop:9000/class6/input/emp hdfs://hadoop:9000/class6/out1
运行成功后，刷新 CentOS HDFS 中的输出路径 /class6/out1 目录，打开 part-r-00000 文件。

hadoop fs -ls /class6/out1
hadoop fs -cat /class6/out1/part-r-00000

可以看到运行结果：

ACCOUNTING 8750
RESEARCH 6775
SALES 9400
请详细分析一下每一句话。


好的，让我来逐行分析这段代码。

```bash
cd /app/hadoop-1.1.2
```
这里进入/app/hadoop-1.1.2目录。

```bash
hadoop jar1SumDeptSalary.jar Q1SumDeptSalary hdfs://hadoop:9000/class6/input/dept hdfs://hadoop:9000/class6/input/emp hdfs://hadoop:9000/class6/out1
```
这里使用hadoop命令来运行Q1SumDeptSalary程序，并指定部门数据路径、员工数据路径和输出路径三个参数。这里jar参数指定要运行的JAR文件，使用Q1SumDeptSalary参数指定要运行的类名。

```bash
hadoop fs -ls /class6/out1
```
这里使用hadoop fs命令来列出/class6/out1目录下的所有文件。

```bash
hadoop fs -cat /class6/out1/part-r-00000
```
这里使用hadoop命令来查看输出文件part-r-00000的内容。





9.
问题分析

求各个部门的人数和平均工资，需要得到各部门工资总数和部门人数，通过两者相除获取各部门平均工资。首先和问题 1 类似在 Mapper 的 Setup 阶段缓存部门数据，然后在 Mapper 阶段抽取出部门编号和员工工资，利用缓存部门数据把部门编号对应为部门名称，接着在 Shuffle 阶段把传过来的数据处理为部门名称对应该部门所有员工工资的列表，最后在 Reduce 中按照部门归组，遍历部门所有员工，求出总数和员工数，输出部门名称和平均工资。
请详细分析一下。

好的，让我来详细分析一下。

这个问题需要求各个部门的人数和平均工资，需要得到各部门工资总数和部门人数，通过两者相除获取各部门平均工资。为了实现这个功能，可以采用 MapReduce 编程模型，具体实现如下：

1. 在 Mapper 的 Setup 阶段缓存部门数据，这样可以在 Mapper 阶段把部门编号转换为部门名称2. 在 Mapper 阶段，抽取出部门编号和员工工资，利用缓存部门数据把部门编号对应为部门名称。

3. 在 Shuffle 阶段，把传过来的数据处理为部门名称对应该部门所有员工工资的列表。

4. 在 Reduce 阶段，按照部门归组，遍历部门所有员工，求出总数和员工数，输出部门名称和平均工资。

具体实现过程如下：

Mapper 阶段：

1. 在 Mapper 的 Setup 阶段，使用 DistributedCache 技术缓存部门数据，这样可以在 Mapper 阶段把部门编号转换为部门名称。

2. 在 Mapper 的 Map 阶段，抽取出部门编号和员工工资，利用缓存部门数据把部门编号对应为部门名称。

3. 将部门名称和员工工资作为 Mapper 的输出，输出格式为 <Text, DoubleWritable>。

Shuffle 阶段：

1. 在 Shuffle 阶段，按照部门名称对 Mapper 的输出进行排序和分组，得部门名称对应该部门所有员工工资的列表。

Reduce 阶段：

1. 在 Reduce 阶段，遍历部门所有员工，求出总数和员工数，输出部门名称和平均工资。

2. 输出格式为 <Text, Text>，其中 Text 表示部门名称，Text 表示该部门的平均工资通过以上步骤，就可以实现求各个部门的人数和平均工资的功能。






10.
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q2DeptNumberAveSalary extends Configured implements Tool {

    public static class MapClass extends Mapper<LongWritable, Text, Text, Text> {

        // 用于缓存 dept文件中的数据
        private Map<String, String> deptMap = new HashMap<String, String>();
        private String[] kv;

        // 此方法会在Map方法执行之前执行且执行一次
        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
            BufferedReader in = null;
            try {
                // 从当前作业中获取要缓存的文件
                Path[] paths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
                String deptIdName = null;
                for (Path path : paths) {

                    // 对部门文件字段进行拆分并缓存到deptMap中
                    if (path.toString().contains("dept")) {
                        in = new BufferedReader(new FileReader(path.toString()));
                        while (null != (deptIdName = in.readLine())) {

                            // 对部门文件字段进行拆分并缓存到deptMap中
                            // 其中Map中key为部门编号，value为所在部门名称
                            deptMap.put(deptIdName.split(",")[0], deptIdName.split(",")[1]);
                        }
                    }
                }
            } catch (IOException e) {
                e.printStackTrace();
            } finally {
                try {
                    if (in != null) {
                        in.close();
                    }
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

            // 对员工文件字段进行拆分
            kv = value.toString().split(",");

            // map join: 在map阶段过滤掉不需要的数据，输出key为部门名称和value为员工工资
            if (deptMap.containsKey(kv[7])) {
                if (null != kv[5] && !"".equals(kv[5].toString())) {
                    context.write(new Text(deptMap.get(kv[7].trim())), new Text(kv[5].trim()));
                }
            }
        }
    }

    public static class Reduce extends Reducer<Text, Text, Text, Text> {

    public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

            long sumSalary = 0;
            int deptNumber = 0;

            // 对同一部门的员工工资进行求和
            for (Text val : values) {
                sumSalary += Long.parseLong(val.toString());
                deptNumber++;
            }

            // 输出key为部门名称和value为该部门员工工资平均值
    context.write(key, new Text("Dept Number:" + deptNumber + ", Ave Salary:" + sumSalary / deptNumber));
        }
    }

    @Override
    public int run(String[] args) throws Exception {

        // 实例化作业对象，设置作业名称、Mapper和Reduce类
        Job job = new Job(getConf(), "Q2DeptNumberAveSalary");
        job.setJobName("Q2DeptNumberAveSalary");
        job.setJarByClass(Q2DeptNumberAveSalary.class);
        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);

        // 设置输入格式类
        job.setInputFormatClass(TextInputFormat.class);

        // 设置输出格式类
        job.setOutputFormatClass(TextOutputFormat.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        // 第1个参数为缓存的部门数据路径、第2个参数为员工数据路径和第3个参数为输出路径
    String[] otherArgs = new GenericOptionsParser(job.getConfiguration(), args).getRemainingArgs();
        DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(), job.getConfiguration());
        FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));

        job.waitForCompletion(true);
        return job.isSuccessful() ? 0 : 1;
    }

    /**
     * 主方法，执行入口
     * @param args 输入参数
     */
    public static void main(String[] args) throws Exception {
        int res = ToolRunner.run(new Configuration(), new Q2DeptNumberAveSalary(), args);
        System.exit(res);
    }
}
请详细分析一下每一句代码。

好的，让我来逐行分析一下代码。

1. 导入需要的包：

```
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
```

2. 定义Mapper类：

```
public static class MapClass extends Mapper<LongWritable, Text, Text, Text> {
```

这里定义了一个Mapper类，继承自Mapper类，并指定了输入键、输入值、输出键和输出值的类型。

3. 定义缓存变量和部门数据读取方法：

``private Map<String, String> deptMap = new HashMap<String, String>();
private String[] kv;

@Override
protected void setup(Context context) throws IOException, InterruptedException {
    BufferedReader in = null;
    try {
        Path[] paths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
        String deptIdName = null;
        for (Path path : paths) {
            if (path.toString().contains("dept")) {
                in = new BufferedReader(new FileReader(path.toString()));
                while (null != (deptIdName = in.readLine())) {
                    deptMap.put(deptIdName.split(",")[0], deptIdName.split(",")[1]);
                }
            }
        }
    } catch (IOException e) {
        e.printStackTrace();
    } finally {
        try {
            if (in != null) {
                in.close();
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```

这里定义了一个Map类型的变量deptMap，用于缓存部门数据。在setup()方法中，首先通过DistributedCache.getLocalCacheFiles()方法获取缓存的文件路径，然后读取部门数据文件，将部门编号和部门名称存储到deptMap中。

4. 定义Mapper类的map()方法：

```
public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
    kv = value.toString().split(",");
    if (deptMap.containsKey(kv[7])) {
        if (null != kv[5] && !"".equals(kv[5].toString())) {
            context.write(new Text(deptMap.get(kv[7].trim())), new Text(kv[5].trim()));
        }
    }
}
```

这里定义了Mapper类的map()方法，用于将员工数据中的部门编号转换为部门名称，并输出部门名称和员工工资。在map()方法中，首先将员工数据按照逗号进行拆分，然后判断部门编号是否在deptMap中存在，如果存在，则将部门编号转换为部门名称，并将部门名称作为key，员工工资作为value，输出到Reducer。

5. 定义Reducer类：

```
public static class Reduce extends Reducer<Text, Text, Text, Text> {
```

这里定义了一个Reducer类，继承自Reducer类，并指定了输入键、输入值、输出键和输出值的类型。

6. 定义Reducer类的reduce()方法：

```
 void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
    long sumSalary = 0;
    int deptNumber = 0;
    for (Text val : values) {
        sumSalary += Long.parseLong(val.toString());
        deptNumber++;
    }
    context.write(key, new Text("Dept Number:" + deptNumber + ", Ave Salary:" + sumSalary / deptNumber));
}
```

这里定义了Reducer类的reduce()方法，用于计算每个部门的平均工资，并输出部门名称和平均工资。在reduce()方法中，首先遍历同一部门的所有员工工资，出该部门的工资总数和员工人数，然后计算平均工资，并将部门名称和平均工资输出。

7.现Tool接口：

```
public class Q2DeptNumberAveSalary extends Configured implements Tool {
```

这里定义了一个类Q2DeptNumberAveSalary，实现了Tool接口，用于处理命令行参数和运行作业。

8. 实现run()方法：

```
@Override
public int run(String[] args) throws Exception {
    Job job = new Job(getConf(), "Q2DeptNumberAveSalary");
    jobJobName("Q2DeptNumberAveSalary");
    job.setJarByClass(Q2DeptNumberAveSalary.class);
    job.setMapperClass(MapClass.class);
    job.setReducerClass(Reduce.class);
    job.setInputFormatClass(TextInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    String[] otherArgs = new GenericOptionsParser(job.getConfiguration(), args).getRemainingArgs();
    DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(), job.getConfiguration());
    FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));
    job.waitForCompletion(true);
    return job.isSuccessful() ? 0 : 1;
}
```

这里实现了Tool接口的run()方法，用于运行作业。在run()方法中，首先实例化一个Job对象，并设置作业名称、Mapper和Reducer类。然后设置输入格式、输出格式类、输出键类型和输出值类型。接着，从命令行参数中获取缓存的部门数据路径、员工数据路径和输出路径，并将部门数据文件添加到istributedCache中。最后，运行作业并返回作业执行结果。

9. 实现main()方法：

```
public static void main(String[] args) throws Exception {
    int res = ToolRunner.run(new Configuration(), new Q2DeptNumberAveSalary(), args);
    System.exit(res);
}
```

这里实现了main()方法，用于启动作业。在main()方法中，首先调用ToolRunner.run()方法运行作业将作业执行结果保存到res变量中。然后，根据作业执行结果退出程序。






11.
3.2.4 编译并打包代码

进入 /app/hadoop-1.1.2/myclass/class6 目录中新建 Q2DeptNumberAveSalary.java 程序代码（代码也可以使用 /home/shiyanlou/install-pack/class6/Q2DeptNumberAveSalary.java 文件）。

cd /app/hadoop-1.1.2/myclass/class6
vi Q2DeptNumberAveSalary.java

编译代码：

javac -classpath ../../hadoop-core-1.1.2.jar:../../lib/commons-cli-1.2.jar Q2DeptNumberAveSalary.java

把编译好的代码打成 jar 包，如果不打成 jar 形式运行会提示 class 无法找到的错误。

jar cvf ./Q2DeptNumberAveSalary.jar ./Q2DeptNum*.class
mv *.jar ../..
rm Q2DeptNum*.class
请结合详细分析一下每一句话


好的，让我来逐行分析一下代码。

1. 进入 /app/hadoop-1.1.2/myclass/class6 目录中新建 Q2DeptNumberAveSalary.java 程序代码（代码也可以使用 /home/shiyanlou/install-pack/class6/QDeptNumberAveSalary.java 文件）。

```
cd /app/hadoop-1.1.2/myclass/class6
vi Q2DeptNumberAveSalary.java
```

这里进入指定目录，并使用vi编辑器创建一个名为Q2DeptNumberAveSalary.java的文件，用于存储程序代码。

2. 编译代码：

```
javac -classpath ../../adoop-core-1.1.2.jar:../../lib/commons-cli-1.2.jar Q2DeptNumberAveSalary.java
```

这里使用javac命令编译Q2NumberAveSalary.java文件，并指定类路径为../../hadoop-core-1.1.2.jar:../../lib/commons-cli-1.2.jar，以便编译器能够找到所需的类。

3. 把编译好的代码打成 jar 包，如果不打成 jar 形式运行会提示 class 无法找到的错误。

``jar cvf ./Q2DeptNumberAveSalary.jar ./Q2DeptNum*.class
```

这里使用jar命令将编译好的Q2DeptNumberAveSalary.class文件打包成Q2DeptNumberAveSalary.jar文件，并指定打包的文件名和文件路径。注意，这里使用了通配符*，以便打包所有以Q2DeptNum开头的class文件。

4. 将打包好的jar文件移动到上一级目录。

```
mv *.jar ../..
```

这里使用mv命令将打包好的jar文件移动到上一级目录。

5. 删除编译好的class文件。

```
rm Q2DeptNum*.class
```

这里使用rm命令删除编译好的Q2DeptNum.class文件，以便下次重新编译。






12.






13.






14.






15.






16.






17.







18.







19.








20.








21.








22.








23.








24.








25.








26.








27.









28.








29.








30.








31.









32.








33.








34.








35.








36.








37.









38.









39.







40.








41.









42.








43.








44.








45.








46.









47.








48.









49.








50.












51.





52.





53.







54.






55.






56.






57.






58.






59.








60.







61.






62.









63.







64.








65.






66.






67.







68.






69.






70.







71.







72.








73.







74.






75.






76.







77.






78.






79.






80.







81.







82.








83.







84.








85.







86.






87.








88.






89.







90.






91.






92.






93.







94.







95.







96.






97.






98.






99.






100.



